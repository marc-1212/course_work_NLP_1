{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQaGMPUu5vQW",
        "outputId": "6d966be6-5f56-4ce0-c7c5-085021a76b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-12 17:12:27--  https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com/cd/0/inline/CFefwgSE3xW4d3EBhNLhhuccuhS_2SNh-_l7k-d_aJwcEMLMUj3scei-qk25k2IGsv29ftJSz2CUcWFeP1OnjnYSGvutPtDN_GvTlEkqwRR0_38vg9TWws-waI6hOauaTSz6AU9ilQ3KNzxfDN33p8Lc/file?dl=1# [following]\n",
            "--2023-10-12 17:12:28--  https://ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com/cd/0/inline/CFefwgSE3xW4d3EBhNLhhuccuhS_2SNh-_l7k-d_aJwcEMLMUj3scei-qk25k2IGsv29ftJSz2CUcWFeP1OnjnYSGvutPtDN_GvTlEkqwRR0_38vg9TWws-waI6hOauaTSz6AU9ilQ3KNzxfDN33p8Lc/file?dl=1\n",
            "Resolving ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com (ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com)... 162.125.64.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com (ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CFdAjvFg9kDw_k9Vui0rQD4ZAM9xLWlug7WA45ItGgObiEKyVtEyIUOsieZH-Ufb6X68-ex_rzj7WhRgzn2uOyQk-iX2oJgwJcqHweMvG6p_THuqi5tmWx9CSBqJlu7hVmYfCbOUi6sZBu6vQ_Q_MLjIzgiji7vlmAayFf67xJWuQVP-XAO3WB4XO1LrqqFtqs3IfPK1Dat_ONpR1vdctzilG0jZ_RROCVGDhDvouiCwlnA4W2n06w6u4RIujfsm1txMS-DPnK1c7SFLsYZXoQoW1Da--VgO93iynYmrL4j2sW9n3VZ3XtJUEewplZDzgKlyifNu5ZTASAH3UnMcJMnn07agF42o-8zFVBV2qQCuMFXN40bN5rfQr1Rzv7BZ_k0/file?dl=1 [following]\n",
            "--2023-10-12 17:12:28--  https://ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com/cd/0/inline2/CFdAjvFg9kDw_k9Vui0rQD4ZAM9xLWlug7WA45ItGgObiEKyVtEyIUOsieZH-Ufb6X68-ex_rzj7WhRgzn2uOyQk-iX2oJgwJcqHweMvG6p_THuqi5tmWx9CSBqJlu7hVmYfCbOUi6sZBu6vQ_Q_MLjIzgiji7vlmAayFf67xJWuQVP-XAO3WB4XO1LrqqFtqs3IfPK1Dat_ONpR1vdctzilG0jZ_RROCVGDhDvouiCwlnA4W2n06w6u4RIujfsm1txMS-DPnK1c7SFLsYZXoQoW1Da--VgO93iynYmrL4j2sW9n3VZ3XtJUEewplZDzgKlyifNu5ZTASAH3UnMcJMnn07agF42o-8zFVBV2qQCuMFXN40bN5rfQr1Rzv7BZ_k0/file?dl=1\n",
            "Reusing existing connection to ucbb9ebe35f10aa8836fb001740a.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191859755 (183M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-full.zip’\n",
            "\n",
            "                     50%[=========>          ]  92.04M  11.6MB/s    eta 8s     ^C\n",
            "--2023-10-12 17:12:38--  https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com/cd/0/inline/CFdSQMnEhfOqNg4y9tpcjr1fFMbCthaJwmWJUeWPNN2XJH3t-81-VMKjBaDXOt48t9K4peqWdSPcEs9MulDYRtGPWR_3rliKSeRmBzm76Dpd0M7QfoWDduUIEJP8bEBmSK2zzCUeY80Lw2EsksTl22Ig/file?dl=1# [following]\n",
            "--2023-10-12 17:12:39--  https://ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com/cd/0/inline/CFdSQMnEhfOqNg4y9tpcjr1fFMbCthaJwmWJUeWPNN2XJH3t-81-VMKjBaDXOt48t9K4peqWdSPcEs9MulDYRtGPWR_3rliKSeRmBzm76Dpd0M7QfoWDduUIEJP8bEBmSK2zzCUeY80Lw2EsksTl22Ig/file?dl=1\n",
            "Resolving ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com (ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com)... 162.125.64.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com (ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CFdMbAL8aQPPpNebyVwGUtwBoYy9GxQtdAqP-AbUtpH6CdNRLOzbccxRJweFHwuSu47JyR-9qBzZPZjVsFFBIA5TX4KnPI-01UGIkS0Bh3kzRA5LiDtgBW73MCMfy8r6UXU_3bGH7O-ozeCU_gUE51fuj1kHwxBbN-LXvCajNfWpN-T693tAHZxnCfJK6XbrNkdgbxK4oR2kRMD6jFx0_7nUbhrv5qr8q-RjN2RTFPRWTvObaU6pCl1xNhJYN_LqiBCcdlwh_8-9GT5vxVdDfUd_RfUaoBBsMSB2-0QcOrksFT8sfCmPVGftdF6gj7MtX9ZQDlXt3ebCGGvnuUjK6MtczexFmmr67L7XlrIbQ85zJomPezNPk-WImmfXd7Riby4/file?dl=1 [following]\n",
            "--2023-10-12 17:12:39--  https://ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com/cd/0/inline2/CFdMbAL8aQPPpNebyVwGUtwBoYy9GxQtdAqP-AbUtpH6CdNRLOzbccxRJweFHwuSu47JyR-9qBzZPZjVsFFBIA5TX4KnPI-01UGIkS0Bh3kzRA5LiDtgBW73MCMfy8r6UXU_3bGH7O-ozeCU_gUE51fuj1kHwxBbN-LXvCajNfWpN-T693tAHZxnCfJK6XbrNkdgbxK4oR2kRMD6jFx0_7nUbhrv5qr8q-RjN2RTFPRWTvObaU6pCl1xNhJYN_LqiBCcdlwh_8-9GT5vxVdDfUd_RfUaoBBsMSB2-0QcOrksFT8sfCmPVGftdF6gj7MtX9ZQDlXt3ebCGGvnuUjK6MtczexFmmr67L7XlrIbQ85zJomPezNPk-WImmfXd7Riby4/file?dl=1\n",
            "Reusing existing connection to ucbf4d10b2961199b81b0445ebfa.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2230160 (2.1M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-10k.zip’\n",
            "\n",
            "wikitext-filtered-1 100%[===================>]   2.13M  12.9MB/s    in 0.2s    \n",
            "\n",
            "2023-10-12 17:12:40 (12.9 MB/s) - ‘wikitext-filtered-10k.zip’ saved [2230160/2230160]\n",
            "\n",
            "--2023-10-12 17:12:41--  https://www.dropbox.com/scl/fi/duwtwt64uzv504if590sf/visa_outliers_US.csv?rlkey=6nictvov5pjaeue3yatz7m83k&dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com/cd/0/inline/CFdmz0vYlf2OU0NFvC37Ik9dLsIU5GR_ONFPghDBy2tN8pYy-5OLuZciY5zp5ojRYLoddWdIT4WO4nahMb4KE497H082a-Df3prLoaFth13C2poXopVL_WW0xjCMniLrwRmSdBdqv9RpjJG2wb4UeBfI/file# [following]\n",
            "--2023-10-12 17:12:42--  https://uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com/cd/0/inline/CFdmz0vYlf2OU0NFvC37Ik9dLsIU5GR_ONFPghDBy2tN8pYy-5OLuZciY5zp5ojRYLoddWdIT4WO4nahMb4KE497H082a-Df3prLoaFth13C2poXopVL_WW0xjCMniLrwRmSdBdqv9RpjJG2wb4UeBfI/file\n",
            "Resolving uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com (uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com)... 162.125.64.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com (uc8af91c5b5c6b4a8ddc5444b333.dl-eu.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4596 (4.5K) [text/plain]\n",
            "Saving to: ‘visa-outlier-clusters.csv’\n",
            "\n",
            "visa-outlier-cluste 100%[===================>]   4.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-12 17:12:43 (302 MB/s) - ‘visa-outlier-clusters.csv’ saved [4596/4596]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\"\n",
        "!wget -O visa-outlier-clusters.csv \"https://www.dropbox.com/scl/fi/duwtwt64uzv504if590sf/visa_outliers_US.csv?rlkey=6nictvov5pjaeue3yatz7m83k&dl=0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  wikitext-filtered-full.zip\n",
            "replace wikitext-filtered-full/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
            "Archive:  wikitext-filtered-10k.zip\n",
            "replace wikitext-filtered-10k/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#!unzip wikitext-filtered-full.zip\n",
        "#!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/marc/.local/lib/python3.8/site-packages (2.14.5)\n",
            "Requirement already satisfied: pandas in /home/marc/.local/lib/python3.8/site-packages (from datasets) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: multiprocess in /home/marc/.local/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /home/marc/.local/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: aiohttp in /home/marc/.local/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: packaging in /home/marc/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/marc/.local/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/marc/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/marc/.local/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/marc/.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/marc/.local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marc/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /home/marc/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/marc/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/marc/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/marc/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYe6Rq5k6Bgu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/marc/.local/lib/python3.8/site-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/marc/.local/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /home/marc/.local/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: tqdm in /home/marc/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /home/marc/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cleaning dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/marc/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Download stop words from the library 'NLTK'\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "# Removing characters non alphabetic\n",
        "wikitext_small = [re.sub(\"[^A-Za-z']+\", \" \", sentence[\"text\"]) for sentence in wikitext_small]\n",
        "wikitext_large  = [re.sub(\"[^A-Za-z']+\", \" \", sentence[\"text\"]) for sentence in wikitext_large]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "#tokenize the words of the two differents datasets\n",
        "wikitext_small_tokenized = [[token for token in gensim.utils.tokenize(sentence.lower(),  deacc = True) if not token in stop_words]\n",
        "                             for sentence in wikitext_small]\n",
        "\n",
        "wikitext_large_tokenized = [[token for token in gensim.utils.tokenize(sentence.lower(),  deacc = True) if not token in stop_words]\n",
        "                             for sentence in wikitext_large]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define hyperparameters variables\n",
        "\n",
        "context_window = 5 # The distance of the words to take in account     \n",
        "\n",
        "vector_size = 50 #how many dimensions is going to have the resultant vector of each word. This vector capture the relationships between words\n",
        "\n",
        "# note: in the coursework pdf appears as 'iter', but it is deprecated and has been changed to 'epochs'.\n",
        "epochs = 5  #how many times the model will go over the entire dataset.\n",
        "\n",
        "min_count = 5  #how many times has the word appears in the dataset to to take in account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "try:\n",
        "    small_df_model = Word2Vec.load('small_df.model')\n",
        "    large_df_model = Word2Vec.load('large_df_model')\n",
        "except:\n",
        "#training models\n",
        "    print(\"models not found\")\n",
        "    small_df_model = Word2Vec(sentences=wikitext_small_tokenized,vector_size=vector_size, window=context_window, min_count=min_count, epochs=epochs)\n",
        "    small_df_model.save(\"small_df.model\")\n",
        "    large_df_model = Word2Vec(sentences=wikitext_large_tokenized, vector_size=vector_size, window=context_window, min_count=min_count, epochs=epochs)\n",
        "    large_df_model.save(\"large_df_model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STEP 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "#crearting a function to print the cousines similarities\n",
        "def print_cousine_similarities(first_word, second_word):\n",
        "    print(f\"{first_word}/{second_word}  \\n\", \n",
        "          \"\\tsmall dataset:\", f\"{small_df_model.wv.similarity(first_word, second_word)}\\n\",\n",
        "            \"\\tlarge datset:\", f\"{large_df_model.wv.similarity(first_word, second_word)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plane/car  \n",
            " \tsmall dataset: 0.9873542785644531\n",
            " \tlarge datset: 0.5832024216651917\n",
            "\n",
            "planet/sun  \n",
            " \tsmall dataset: 0.993909478187561\n",
            " \tlarge datset: 0.6373756527900696\n",
            "\n",
            "cup/article  \n",
            " \tsmall dataset: 0.6564936637878418\n",
            " \tlarge datset: 0.05309109389781952\n",
            "\n",
            "sugar/approach  \n",
            " \tsmall dataset: 0.9856008291244507\n",
            " \tlarge datset: -0.21134068071842194\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_cousine_similarities(\"plane\", \"car\")\n",
        "print_cousine_similarities(\"planet\", \"sun\")\n",
        "print_cousine_similarities(\"cup\", \"article\")\n",
        "print_cousine_similarities(\"sugar\", \"approach\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "other words similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "car/truck  \n",
            " \tsmall dataset: 0.9762018322944641\n",
            " \tlarge datset: 0.8513953685760498\n",
            "\n",
            "love/sex  \n",
            " \tsmall dataset: 0.7270536422729492\n",
            " \tlarge datset: 0.5573849678039551\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('prince', 0.6806843876838684),\n",
              " ('tsar', 0.6511449813842773),\n",
              " ('son', 0.6379523277282715),\n",
              " ('regent', 0.637367844581604),\n",
              " ('walsingham', 0.6260783076286316),\n",
              " ('deathbed', 0.6223783493041992),\n",
              " ('isabeau', 0.6184468269348145),\n",
              " ('hardinge', 0.6021685004234314),\n",
              " ('deed', 0.5984432697296143),\n",
              " ('throne', 0.5981664657592773)]"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print_cousine_similarities(\"car\", \"truck\")\n",
        "print_cousine_similarities(\"love\", \"sex\")\n",
        "large_df_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"male\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STEP 4:     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Human (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>love</td>\n",
              "      <td>sex</td>\n",
              "      <td>6.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>book</td>\n",
              "      <td>paper</td>\n",
              "      <td>7.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>computer</td>\n",
              "      <td>keyboard</td>\n",
              "      <td>7.62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Word 1    Word 2  Human (mean)\n",
              "0      love       sex          6.77\n",
              "1     tiger       cat          7.35\n",
              "2     tiger     tiger         10.00\n",
              "3      book     paper          7.46\n",
              "4  computer  keyboard          7.62"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "\n",
        "wordsim353_df = pd.read_csv('combined.csv')\n",
        "wordsim353_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "353\n",
            "missing values of small_df: 101\n",
            "missing values of large_df: 18\n"
          ]
        }
      ],
      "source": [
        "#function to add the cousine correlation in the dataset\n",
        "import numpy as np\n",
        "\n",
        "def add_cousin_correlation(df : pd.DataFrame, name_label: str, model: Word2Vec):\n",
        "    score = []\n",
        "    missing_values = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if row[\"Word 1\"]  in model.wv.index_to_key and row[\"Word 2\"] in model.wv.index_to_key:\n",
        "            score.append( model.wv.similarity(row[\"Word 1\"], row[\"Word 2\"]))\n",
        "        else:\n",
        "            missing_values += 1\n",
        "            score.append(np.nan)\n",
        "    df[name_label] = score\n",
        "    print(f\"missing values of {name_label}: {missing_values}\")\n",
        "\n",
        "add_cousin_correlation(wordsim353_df, \"small_df\", small_df_model )\n",
        "add_cousin_correlation(wordsim353_df, \"large_df\", large_df_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Human (mean)</th>\n",
              "      <th>small_df</th>\n",
              "      <th>large_df</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>love</td>\n",
              "      <td>sex</td>\n",
              "      <td>6.77</td>\n",
              "      <td>0.727054</td>\n",
              "      <td>0.557385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "      <td>0.994153</td>\n",
              "      <td>0.610930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>book</td>\n",
              "      <td>paper</td>\n",
              "      <td>7.46</td>\n",
              "      <td>0.831142</td>\n",
              "      <td>0.580514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>computer</td>\n",
              "      <td>keyboard</td>\n",
              "      <td>7.62</td>\n",
              "      <td>0.982206</td>\n",
              "      <td>0.422520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Word 1    Word 2  Human (mean)  small_df  large_df\n",
              "0      love       sex          6.77  0.727054  0.557385\n",
              "1     tiger       cat          7.35  0.994153  0.610930\n",
              "2     tiger     tiger         10.00  1.000000  1.000000\n",
              "3      book     paper          7.46  0.831142  0.580514\n",
              "4  computer  keyboard          7.62  0.982206  0.422520"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#clean the df\n",
        "clean_small_wordsim353_df = wordsim353_df.dropna(subset=['small_df'], inplace=False)\n",
        "clean_large_wordsim353_df = wordsim353_df.dropna(subset=['large_df'], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(SignificanceResult(statistic=0.10142118986984747, pvalue=0.1082406979417225),\n",
              " SignificanceResult(statistic=0.627196693222107, pvalue=4.979199074045186e-38))"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#calculate the spearman correlation\n",
        "\n",
        "small_correlation = spearmanr(clean_small_wordsim353_df['Human (mean)'], clean_small_wordsim353_df[\"small_df\"])\n",
        "large_correlation = spearmanr(clean_large_wordsim353_df['Human (mean)'], clean_large_wordsim353_df[\"large_df\"])\n",
        "small_correlation, large_correlation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
